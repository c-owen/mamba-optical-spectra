import json
import logging
from pathlib import Path
from typing import Dict, Optional, Tuple

import numpy as np
import torch
from torch.utils.data import DataLoader, Dataset


logger = logging.getLogger(__name__)


class SpectraDataset(Dataset):
    """
    PyTorch Dataset for synthetic optical spectra.

    Expects files generated by src/data/generate_spectra.py, specifically:
        - X_{split}.npy  (shape: [N, num_points])
        - y_{split}.npy  (shape: [N])
        - wavelength_axis.npy
        - metadata.json

    Returns:
        spectrum: FloatTensor of shape [1, num_points]
        label:    LongTensor scalar
    """

    def __init__(
        self,
        split: str,
        data_dir: Path | str = "data/processed",
        normalize: bool = True,
    ) -> None:
        """
        Args:
            split: "train", "val", or "test"
            data_dir: directory containing the .npy files and metadata.json
            normalize: if True, apply per-sample standardization
        """
        super().__init__()

        if split not in {"train", "val", "test"}:
            raise ValueError(f"Invalid split '{split}'. Expected 'train', 'val', or 'test'.")

        self.split = split
        self.data_dir = Path(data_dir)
        self.normalize = normalize

        X_path = self.data_dir / f"X_{split}.npy"
        y_path = self.data_dir / f"y_{split}.npy"
        lam_path = self.data_dir / "wavelength_axis.npy"
        meta_path = self.data_dir / "metadata.json"

        if not X_path.exists() or not y_path.exists():
            raise FileNotFoundError(
                f"Could not find X/y files for split '{split}' under '{self.data_dir}'. "
                f"Expected {X_path} and {y_path}."
            )

        logger.info("Loading split '%s' from '%s'", split, self.data_dir)
        self.X = np.load(X_path)  # shape: [N, L]
        self.y = np.load(y_path)  # shape: [N]

        if self.X.ndim != 2:
            raise ValueError(f"Expected X to have shape [N, L], got {self.X.shape}")
        if self.y.ndim != 1:
            raise ValueError(f"Expected y to have shape [N], got {self.y.shape}")
        if self.X.shape[0] != self.y.shape[0]:
            raise ValueError(f"Mismatched X and y shapes: {self.X.shape}, {self.y.shape}")

        # Optional: load wavelength axis for reference (not used by models directly)
        if lam_path.exists():
            self.lam = np.load(lam_path)
            if self.lam.shape[0] != self.X.shape[1]:
                logger.warning(
                    "Wavelength axis length (%d) does not match spectra length (%d).",
                    self.lam.shape[0],
                    self.X.shape[1],
                )
        else:
            self.lam = None
            logger.warning("wavelength_axis.npy not found in '%s'.", self.data_dir)

        # Load metadata for class mapping
        if meta_path.exists():
            with open(meta_path, "r", encoding="utf-8") as f:
                meta = json.load(f)
            self.class_index_to_name = {int(k): v for k, v in meta["class_index_to_name"].items()}
            self.num_classes = int(meta["num_classes"])
        else:
            self.class_index_to_name = None
            self.num_classes = int(self.y.max()) + 1
            logger.warning("metadata.json not found in '%s'. Inferring num_classes from y.", self.data_dir)

        logger.info(
            "Loaded split '%s': X shape=%s, y shape=%s, num_classes=%d",
            split,
            self.X.shape,
            self.y.shape,
            self.num_classes,
        )

    def __len__(self) -> int:
        return self.X.shape[0]

    def _normalize_sample(self, x: np.ndarray) -> np.ndarray:
        """
        Per-sample standardization: (x - mean) / std.
        Avoids division by zero by adding a small epsilon.
        """
        if not self.normalize:
            return x

        mean = x.mean()
        std = x.std()
        if std < 1e-8:
            return x - mean
        return (x - mean) / std

    def __getitem__(self, idx: int) -> Tuple[torch.Tensor, torch.Tensor]:
        """
        Returns:
            spectrum: FloatTensor of shape [1, num_points]
            label:    LongTensor scalar
        """
        x = self.X[idx]  # shape: [num_points]
        y = self.y[idx]  # scalar

        x = self._normalize_sample(x)

        # Convert to tensors; add channel dimension for Conv1d: [1, L]
        x_tensor = torch.from_numpy(x.astype(np.float32)).unsqueeze(0)
        y_tensor = torch.tensor(y, dtype=torch.long)

        return x_tensor, y_tensor


def get_class_mapping(data_dir: Path | str = "data/processed") -> Dict[int, str]:
    """
    Load the class index to name mapping from metadata.json.

    Returns:
        dict: {class_index: class_name}
    """
    data_dir = Path(data_dir)
    meta_path = data_dir / "metadata.json"

    if not meta_path.exists():
        raise FileNotFoundError(f"metadata.json not found in '{data_dir}'")

    with open(meta_path, "r", encoding="utf-8") as f:
        meta = json.load(f)

    mapping = {int(k): v for k, v in meta["class_index_to_name"].items()}
    logger.info("Loaded class mapping from '%s': %s", meta_path, mapping)
    return mapping


def get_dataloaders(
    data_dir: Path | str = "data/processed",
    batch_size: int = 64,
    num_workers: int = 0,
    normalize: bool = True,
    pin_memory: bool = True,
    persistent_workers: bool = False,
) -> Dict[str, DataLoader]:
    """
    Convenience function to create train/val/test dataloaders.

    Args:
        data_dir: directory containing processed dataset.
        batch_size: batch size for all splits.
        num_workers: DataLoader workers (0 is fine for a small project).
        normalize: whether to apply per-sample normalization.
        pin_memory: whether to pin memory for faster GPU transfers.
        persistent_workers: passed through to DataLoader.

    Returns:
        dict: {"train": train_loader, "val": val_loader, "test": test_loader}
    """
    data_dir = Path(data_dir)
    logger.info(
        "Creating dataloaders from '%s' with batch_size=%d, num_workers=%d, normalize=%s",
        data_dir,
        batch_size,
        num_workers,
        normalize,
    )

    train_dataset = SpectraDataset(split="train", data_dir=data_dir, normalize=normalize)
    val_dataset = SpectraDataset(split="val", data_dir=data_dir, normalize=normalize)
    test_dataset = SpectraDataset(split="test", data_dir=data_dir, normalize=normalize)

    train_loader = DataLoader(
        train_dataset,
        batch_size=batch_size,
        shuffle=True,
        num_workers=num_workers,
        pin_memory=pin_memory,
        persistent_workers=persistent_workers if num_workers > 0 else False,
    )

    val_loader = DataLoader(
        val_dataset,
        batch_size=batch_size,
        shuffle=False,
        num_workers=num_workers,
        pin_memory=pin_memory,
        persistent_workers=persistent_workers if num_workers > 0 else False,
    )

    test_loader = DataLoader(
        test_dataset,
        batch_size=batch_size,
        shuffle=False,
        num_workers=num_workers,
        pin_memory=pin_memory,
        persistent_workers=persistent_workers if num_workers > 0 else False,
    )

    logger.info(
        "DataLoaders created: train_batches=%d, val_batches=%d, test_batches=%d",
        len(train_loader),
        len(val_loader),
        len(test_loader),
    )

    return {
        "train": train_loader,
        "val": val_loader,
        "test": test_loader,
    }
